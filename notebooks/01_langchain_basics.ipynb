{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288ce9a8",
   "metadata": {},
   "source": [
    "# LangChain Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e108151",
   "metadata": {},
   "source": [
    "## LangChain Main Concepts:\n",
    "\n",
    "- LangChain Components\n",
    "- Chains (Simple, Sequential, Custom)\n",
    "- Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec206f49",
   "metadata": {},
   "source": [
    "### Preliminary Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the required libraries\n",
    "!pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183c80d",
   "metadata": {},
   "source": [
    "Le librerie da importare sono:\n",
    "- openai\n",
    "- langchain\n",
    "- pinecone-client\n",
    "- python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2f431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14256a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrading langchain\n",
    "!pip install langchain --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4407a3a",
   "metadata": {},
   "source": [
    "### Importiamo le API Keys dal file _.env_ mediante: **python-dotenv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9420415",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# loading the API Keys (OpenAI, Pinecone) from .env\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "os.environ.get('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042c892",
   "metadata": {},
   "source": [
    "## LangChain Components:\n",
    "- LLM Wrappers (LLM and Chat)\n",
    "- Prompt Templates\n",
    "- Indexes\n",
    "- Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29231e2",
   "metadata": {},
   "source": [
    "## LLM Models (Wrappers): GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d37cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'max_tokens': 512, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n"
     ]
    }
   ],
   "source": [
    "# Importo OpenAI e creo un'istanza di una LLM (in questo caso GPT-3)\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=512)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d355809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Quantum mechanics is the branch of physics that describes the behavior of matter and energy at the scale of atoms and subatomic particles.\n"
     ]
    }
   ],
   "source": [
    "# Passo un prompt al LLM e ottengo una risposta\n",
    "output = llm('explain quantum mechanics in one sentence')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e29c5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Verifico il mio prompt quanti token usa\n",
    "print(llm.get_num_tokens('explain quantum mechanics in one sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00248e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Richiesta multipla, mediante lista di prompt\n",
    "output = llm.generate(['... is the capital of France.', \n",
    "                   'What is the formula for the area of a circle?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98fd88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='\\n\\nParis', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe formula for the area of a circle is A = πr², where A is the area, π is 3.14, and r is the radius of the circle.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
     ]
    }
   ],
   "source": [
    "print(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0725e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "# Primo risultato\n",
    "print(output.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondo risultato\n",
    "print(output.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7703b86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779b3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Richiedo 3 possibili risposte per lo stesso prompt\n",
    "output = llm.generate(['Write an orignal tagline for a burger restaurant'] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d3cdfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Taste the Juiciness of our Burgers - Bite Into Burgers!\"\n",
      "\n",
      "\"Making Mouth-Watering Burgers Since Day One!\"\n",
      "\n",
      "\"Experience the Best Burgers in Town - At Burger Haven!\""
     ]
    }
   ],
   "source": [
    "for o in output.generations:\n",
    "    print(o[0].text, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ee709",
   "metadata": {},
   "source": [
    "## Chat Models (Wrappers): GPT-3.5-Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "054eadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo lo \"schema\" dei messaggi\n",
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9bdecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024) #potrei usare anche 'gpt-4'\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='explain quantum mechanics in one sentence')\n",
    "]\n",
    "output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "761e06dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantenmechanik beschreibt das Verhalten von Teilchen auf mikroskopischer Ebene und erlaubt es uns, ihre Eigenschaften und Wechselwirkungen zu verstehen.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12214047",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024) #potrei usare anche 'gpt-4'\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in Italian.'),\n",
    "    HumanMessage(content='explain quantum mechanics in one sentence')\n",
    "]\n",
    "output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bed1e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La meccanica quantistica è una teoria fisica che descrive il comportamento delle particelle subatomiche come onde probabilistiche, e che richiede l'uso di equazioni ondulatorie per calcolare le probabilità di misura di osservabili.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ad29b",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a31ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "612ed8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['virus', 'language'] output_parser=None partial_variables={} template='You are an experienced virologist.\\nWrite a few sentences about the following {virus} in {language}.' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following {virus} in {language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b28accc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "L'ebola è una malattia infettiva virale che causa una grave sindrome da sindrome emorragica. Si ritiene che sia stata originariamente trasmessa dai pipistrelli ai primati e poi all'uomo. I sintomi dell'ebola includono febbre alta, mal di testa, dolori muscolari, debolezza e diarrea. La malattia può diventare molto grave, portando a insufficienza renale, insufficienza epatica e sanguinamento interno e esterno.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7)\n",
    "output = llm(prompt.format(virus='ebola', language='Italian'))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d9b84",
   "metadata": {},
   "source": [
    "## Simple Chains (single LLM used to perform a single task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd1e5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following {virus} in {language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "output = chain.run({'virus': 'HSV', 'language': 'italian'})\n",
    "# Nel caso di un solo argomento non serve il dizionario\n",
    "#output = chain.run('HSV')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be42818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'HSV, acronimo di Herpes Simplex Virus, è un virus appartenente alla famiglia degli Herpesviridae. Esistono due tipi principali di HSV: l'HSV-1 e l'HSV-2. L'HSV-1 è comunemente associato all'herpes labiale, mentre l'HSV-2 è responsabile dell'herpes genitale. Entrambi i tipi di HSV possono causare infezioni ricorrenti e dolorose, caratterizzate da vesciche che si formano sulla pelle o sulle mucose. Non esiste una cura definitiva per l'HSV, ma possono essere utilizzati farmaci antivirali per ridurre la durata e la gravità delle infezioni.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283cc13",
   "metadata": {},
   "source": [
    "## SimpleSequentialChains\n",
    "\n",
    "Permettono di fare una serie di calls a uno o più LLMs, l'ouput di una chain è l'input di un'altra.\n",
    "Nelle **Simple** ogni singola catena ha un solo input e output, che costituisce l'input della catena sucessiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d68115d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "def linear_regression(x, y):\n",
      "    \"\"\"\n",
      "    Calculates linear regression of two sets of data points\n",
      "    \n",
      "    Parameters:\n",
      "    x: A list of x values\n",
      "    y: A list of y values\n",
      "    \n",
      "    Returns:\n",
      "    A tuple (m, b) of the slope and intercept of the line of best fit\n",
      "    \"\"\"\n",
      "    # Calculate the means\n",
      "    x_mean = sum(x) / len(x)\n",
      "    y_mean = sum(y) / len(y)\n",
      "\n",
      "    # Calculate the numerator and denominator of the slope\n",
      "    numerator = 0\n",
      "    denominator = 0\n",
      "    for i in range(len(x)):\n",
      "        numerator += (x[i] - x_mean) * (y[i] - y_mean)\n",
      "        denominator += (x[i] - x_mean) ** 2\n",
      "    \n",
      "    # Calculate the slope and intercept\n",
      "    m = numerator / denominator\n",
      "    b = y_mean - m * x_mean\n",
      "    \n",
      "    return (m, b)\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe given Python function named \"linear_regression\" accepts two lists named \"x\" and \"y\" as input parameters. These lists represent the x-values and y-values of a set of data points, respectively. \n",
      "\n",
      "The function calculates the linear regression of the data points, which involves finding the line of best fit that minimizes the sum of squared distances between the points and the line. \n",
      "\n",
      "In order to calculate the line of best fit, the function first calculates the mean (average) of the x-values and the y-values. This is done by summing all the values in the respective list and dividing the sum by the length of the list.\n",
      "\n",
      "Next, the function initializes numerator and denominator variables to zero. These variables will be used to calculate the slope of the line of best fit. \n",
      "\n",
      "The function then enters a loop that iterates over each index of the x list. On each iteration, it subtracts the x-mean from the current x-value, and multiplies that by the corresponding difference between the y-value and the y-mean. These values are added to the numerator variable.\n",
      "\n",
      "In the same loop, it squares the difference between the x-value and the x-mean and adds it to the denominator variable.\n",
      "\n",
      "After the loop completes, the function divides the numerator by the denominator to calculate the slope of the line, which represents the change in y divided by the change in x.\n",
      "\n",
      "Finally, the function calculates the y-intercept of the line by subtracting the product of the slope and the x-mean from the y-mean.\n",
      "\n",
      "The function returns the calculated slope and intercept as a tuple (m, b), representing the line of best fit.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=1024)\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['concept'],\n",
    "    template='''You are an experienced scientist and Python programmer.\n",
    "    Write a function that implements the concept of {concept}.'''\n",
    ")\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=['function'],\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = overall_chain.run('linear regression') #Si noti che viene passato solo l'argomento della prima catena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a9c39",
   "metadata": {},
   "source": [
    "### LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "631fd9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool  \n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "115e2fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to use the Python REPL to calculate this\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'146306.05007233328'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.run('what is the answer to 5.1 ** 7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f0af1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to calculate the factorial of 20 and then take the square root of that\n",
      "Action: Python_REPL\n",
      "Action Input: from math import factorial; print(round(factorial(20)**0.5, 4))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1559776268.6285\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 1559776268.6285\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1559776268.6285'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run('Calculate the square root of the factorial of 20 and display it with 4 decimal points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8122b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
